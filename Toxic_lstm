import pandas as pd
import string
import numpy as np
import nltk
import random
import chardet
import re
from sklearn.model_selection import train_test_split
from keras.layers import LSTM
from keras.preprocessing import sequence
from numpy import array
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.embeddings import Embedding
from keras.layers import Dropout
# fix random seed for reproducibility
np.random.seed(54621)

train=pd.read_csv('C:/Users/Tirthankar/Desktop/Kaggle/Jigsaw_toxic/train_v1.csv')
train1=train.replace('\n',' ', regex=True)

#Text cleaning Part
train1['cleaned']=train1['comment_text'].str.replace(':\)','emojihappy1')
train1['cleaned']=train1['cleaned'].str.replace(':P','emojihappy2')
train1['cleaned']=train1['cleaned'].str.replace(':p','emojihappy3')
train1['cleaned']=train1['cleaned'].str.replace(':>','emojihappy4')
train1['cleaned']=train1['cleaned'].str.replace(':3','emojihappy5')
train1['cleaned']=train1['cleaned'].str.replace(':D','emojihappy6')
train1['cleaned']=train1['cleaned'].str.replace('XD','emojihappy7')
train1['cleaned']=train1['cleaned'].str.replace('<3','emojihappy8')
train1['cleaned']=train1['cleaned'].str.replace(':\(','emojisad9')
train1['cleaned']=train1['cleaned'].str.replace(':<','emojisad10')
train1['cleaned']=train1['cleaned'].str.replace('>:\(','emojisad11')

train1['cleaned']=train1['cleaned'].str.replace('(@)\w+','mentiontoken')
train1['cleaned']=train1['cleaned'].str.replace('http(s)*:(\S)*','linktoken')

train1['cleaned']=train1['cleaned'].str.replace('[^A-Za-z0-9(),!?\'\`]',' ')
train1['cleaned']=train1['cleaned'].str.replace('[^\w\s]','')


#Toxic train-test creation
model_toxic=train1[['cleaned','toxic']]
X_train, X_test, y_train, y_test = train_test_split(model_toxic["cleaned"], 
                                                    model_toxic['toxic'], test_size=0.90,
                                                    random_state=0)
print(y_test.sum()/y_test.shape[0])
print(y_train.sum()/y_train.shape[0])

#X_train
#Size:15957
#X_test
#size:143614

t1=pd.DataFrame(X_train)
t2=pd.DataFrame(X_test)

test_chk = pd.read_csv('C:/Users/Tirthankar/Desktop/Kaggle/Jigsaw_toxic/test_v2.csv')
test_chk1=test_chk.replace('\n',' ', regex=True)

test_chk1['cleaned']=test_chk1['comment_text'].str.replace(':\)','emojihappy1')
test_chk1['cleaned']=test_chk1['cleaned'].str.replace(':P','emojihappy2')
test_chk1['cleaned']=test_chk1['cleaned'].str.replace(':p','emojihappy3')
test_chk1['cleaned']=test_chk1['cleaned'].str.replace(':>','emojihappy4')
test_chk1['cleaned']=test_chk1['cleaned'].str.replace(':3','emojihappy5')
test_chk1['cleaned']=test_chk1['cleaned'].str.replace(':D','emojihappy6')
test_chk1['cleaned']=test_chk1['cleaned'].str.replace('XD','emojihappy7')
test_chk1['cleaned']=test_chk1['cleaned'].str.replace('<3','emojihappy8')
test_chk1['cleaned']=test_chk1['cleaned'].str.replace(':\(','emojisad9')
test_chk1['cleaned']=test_chk1['cleaned'].str.replace(':<','emojisad10')
test_chk1['cleaned']=test_chk1['cleaned'].str.replace('>:\(','emojisad11')

test_chk1['cleaned']=test_chk1['cleaned'].str.replace('(@)\w+','mentiontoken')
test_chk1['cleaned']=test_chk1['cleaned'].str.replace('http(s)*:(\S)*','linktoken')

test_chk1['cleaned']=test_chk1['cleaned'].str.replace('[^A-Za-z0-9(),!?\'\`]',' ')
test_chk1['cleaned']=test_chk1['cleaned'].str.replace('[^\w\s]','')

#All appended data
df1 = t1.append(t2)
tt=pd.DataFrame(test_chk1['cleaned'])
df=df1.append(tt)
#Size:312735

#Finding out number of unique words
uniqueWords = list(set(" ".join(df.cleaned).lower().split(" ")))
len(uniqueWords)
#306258

#Finding out max number of words in a sentence
count=df['cleaned'].str.split().str.len()
count.index=count.index.astype(str) + 'words:'
max(count)
#2142

vocab_size = 306300
encoded_docs = [one_hot(d, vocab_size) for d in df.cleaned]

train_model= encoded_docs[:15957]
test_model= encoded_docs[15957:159571]
valid_model= encoded_docs[159571:312735]


max_review_length = 2142
train_model = sequence.pad_sequences(train_model, maxlen=max_review_length)
test_model = sequence.pad_sequences(test_model, maxlen=max_review_length)


embedding_vecor_length = 32
model = Sequential()
model.add(Embedding(306300, embedding_vecor_length, input_length=max_review_length))
model.add(Dropout(0.2))
model.add(LSTM(100))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())


model.fit(train_model, y_train, epochs=10, batch_size=2000)
# Final evaluation of the model
scores = model.evaluate(test_model, y_test, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))

t1=model.predict(valid_model)
toxic=pd.DataFrame(t1,columns=['toxic'])




